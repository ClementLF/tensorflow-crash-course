# -*- coding: utf-8 -*-
"""cudnngru_fmnist.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Nn_9cdSK9yH4nWJx-vdKat8NWnmjopu0

# Fashion-MNIST


RNN worflow based on the work of [Aymeric Damien](https://github.com/aymericdamien/TensorFlow-Examples/) and [Sungjoon](https://github.com/sjchoi86/tensorflow-101/blob/master/notebooks/rnn_mnist_simple.ipynb)

Good resource: [How to use Dataset in TensorFlow](https://towardsdatascience.com/how-to-use-dataset-in-tensorflow-c758ef9e4428)

## RNN Overview

<img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png" alt="nn" style="width: 600px;"/>

References:
- [Long Short Term Memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf), Sepp Hochreiter & Jurgen Schmidhuber, Neural Computation 9(8): 1735-1780, 1997.

## System Information
"""

!pip install watermark

# %load_ext watermark
# %watermark -v -m -p tensorflow,numpy -g

# From https://stackoverflow.com/questions/48750199/google-colaboratory-misleading-information-about-its-gpu-only-5-ram-available
# memory footprint support libraries/code
!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi
!pip install gputil
!pip install psutil
!pip install humanize

import psutil
import humanize
import os
import GPUtil as GPU
GPUs = GPU.getGPUs()
# XXX: only one GPU on Colab and isn't guaranteed
gpu = GPUs[0]

def printm():
  process = psutil.Process(os.getpid())
  print("Gen RAM Free: " + humanize.naturalsize( psutil.virtual_memory().available ), " I Proc size: " + humanize.naturalsize( process.memory_info().rss))
  print('GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB'.format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))

printm()

from pathlib import Path
import random 
from datetime import datetime

import tensorflow as tf
from tensorflow.contrib import rnn
import numpy as np
import pandas as pd

!nvidia-smi

!ps -ef|grep python

"""## Prepare Fashon-MNIST
[Data Source](https://www.kaggle.com/zalando-research/fashionmnist/data)
"""

from google.colab import auth
auth.authenticate_user()

# https://cloud.google.com/resource-manager/docs/creating-managing-projects
project_id = 'personal-project-196600 '
!gcloud config set project {project_id}

# Download the file from a given Google Cloud Storage bucket.
!gsutil cp gs://colab-tmp/fashion-mnist_train.csv .
!gsutil cp gs://colab-tmp/fashion-mnist_test.csv .

df_train = pd.read_csv("fashion-mnist_train.csv")
df_test = pd.read_csv("fashion-mnist_test.csv")
print(df_train.shape, df_test.shape)

print(df_train.columns)
print(df_test.columns)

# Train/Validation Split
idx = np.arange(60000)
np.random.shuffle(idx)
print(idx[:5])
df_val = df_train.iloc[idx[:10000]]
df_train = df_train.iloc[idx[10000:]]
print(df_val.shape, df_train.shape)

"""## Build the Model"""

# Training Parameters
learning_rate = 0.002
training_steps = 10000
batch_size = 32
display_step = 500
print("Total batches per epoch:", np.ceil(df_train.shape[0] / batch_size))

# Network Parameters
num_input = 1 # MNIST data input (img shape: 28*28)
timesteps = 28 * 28 # timesteps
num_hidden = 128 # hidden layer num of features
num_classes = 10 # MNIST total classes (0-9 digits)

def RNN(x):
    # Define a lstm cell with tensorflow
    gru = tf.contrib.cudnn_rnn.CudnnGRU(
            1, num_hidden,
            # kernel_initializer=tf.random_normal_initializer(0, 1))
            kernel_initializer=tf.orthogonal_initializer())
            # kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=True))
            # kernel_initializer=tf.truncated_normal_initializer())

    # Get lstm cell output
    # outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)
    outputs, _ = gru(tf.transpose(x, (1, 0, 2)))
    # return tf.matmul(outputs[-1, :, :], weights['out']) + biases['out']
    output_layer = tf.layers.Dense(
        num_classes, activation=None, 
        # kernel_initializer=tf.random_normal_initializer(0, 1),
        kernel_initializer=tf.orthogonal_initializer(),
        # kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=True),
        # kernel_initializer=tf.glorot_uniform_initializer(),
        trainable=True
    )
    # Linear activation, using rnn inner loop last output
    return output_layer(tf.layers.batch_normalization(outputs[-1, :, :]))
    # return output_layer(outputs[-1, :, :])

def process_batch(batch_x, batch_y):
    return tf.expand_dims(batch_x, -1), tf.one_hot(batch_y, num_classes)

tf.reset_default_graph()
graph = tf.Graph()
with graph.as_default():
    tf.set_random_seed(10)
    
    with tf.variable_scope("datasets"):
        training_batch_size = tf.placeholder(tf.int64) # tf.constant(32, dtype="int64")
        inference_batch_size = tf.placeholder(tf.int64) # tf.constant(500, dtype="int64")

        fminst_ds_train = tf.data.Dataset.from_tensor_slices(
            (df_train.iloc[:, 1:].astype("float32") / 255, df_train.iloc[:, 0].astype("int32"))
        ).shuffle(50000, reshuffle_each_iteration=True).repeat().batch(training_batch_size).map(process_batch)
        fminst_ds_val = tf.data.Dataset.from_tensor_slices(
            (df_val.iloc[:, 1:].astype("float32") / 255, df_val.iloc[:, 0].astype("int32"))
        ).repeat().batch(inference_batch_size).map(process_batch)
        fminst_ds_test = tf.data.Dataset.from_tensor_slices((
            df_test.iloc[:, 1:].astype("float32") / 255, df_test.iloc[:, 0].astype("int32"))
        ).repeat().batch(inference_batch_size).map(process_batch)

        handle = tf.placeholder(tf.string, shape=[])
        iterator = tf.data.Iterator.from_string_handle(
            handle, fminst_ds_train.output_types, fminst_ds_train.output_shapes)

        train_iterator = fminst_ds_train.make_initializable_iterator()
        val_iterator = fminst_ds_val.make_initializable_iterator()
        test_iterator = fminst_ds_test.make_initializable_iterator() 
    
    X_0, Y = iterator.get_next()
    X = tf.reshape(X_0, (-1, timesteps, num_input))

    # Define weights
    logits = RNN(X)
    prediction = tf.nn.softmax(logits)
   
    with tf.name_scope("loss"):
        # Define loss and optimizer
        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(
            logits=logits, labels=Y))
        ema_loss = tf.get_variable("ema_loss", shape=[], dtype=tf.float32, trainable=False, initializer=tf.constant_initializer(2.5))
        ema_update = ema_loss.assign(ema_loss * 0.99 + loss * 0.01)
    tf.summary.scalar('Loss', ema_loss)
    
    with tf.variable_scope("optimizer"):
        # optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate)
        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.1)
        gvs = optimizer.compute_gradients(loss)
        capped_gvs = [(tf.clip_by_norm(grad, 1.), var) for grad, var in gvs]
        with tf.control_dependencies([ema_update]):
            train_op = optimizer.apply_gradients(capped_gvs)    
        # train_op = optimizer.minimize(loss_op)

    with tf.variable_scope("inference"):
        # Evaluate model (with test logits, for dropout to be disabled)
        correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))
        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))

    with tf.variable_scope('summarize_gradients'):
        for grad, var in gvs:
            norm = tf.norm(tf.clip_by_norm(grad, 10.), ord=2)
            tf.summary.histogram(var.name.replace(":", "_") + '/gradient_l2', 
                                 tf.where(tf.is_nan(norm), tf.zeros_like(norm), norm))
        for grad, var in capped_gvs:
            norm = tf.norm(grad, ord=2)
            tf.summary.histogram(var.name.replace(":", "_") + '/gradient_clipped_l2', 
                                 tf.where(tf.is_nan(norm), tf.zeros_like(norm), norm)) 
            
    # Initialize the variables (i.e. assign their default value)
    init = tf.global_variables_initializer()
    saver = tf.train.Saver()
    merged_summary_op = tf.summary.merge_all()
    # print([x.get_shape() for x in tf.global_variables()])
    # print([x.get_shape() for x in tf.trainable_variables()])
    # print("All parameters:", np.sum([np.product([xi.value for xi in x.get_shape()]) for x in tf.global_variables()]))
    # print("Trainable parameters:", np.sum([np.product([xi.value for xi in x.get_shape()]) for x in tf.trainable_variables()]))

"""### Training and Evaluating"""

# Start training
config = tf.ConfigProto()
config.gpu_options.allow_growth = True
best_val_acc = 0.8

train_writer = tf.summary.FileWriter("logs/fminst_gru/%s/train" % datetime.now().strftime("%Y%m%d_%H%M"), graph)
val_writer = tf.summary.FileWriter("logs/fminst_gru/%s/val" % datetime.now().strftime("%Y%m%d_%H%M"))

with tf.Session(graph=graph, config=config) as sess:
    sess.run(init) 
    sess.run([train_iterator.initializer, val_iterator.initializer, test_iterator.initializer],
             feed_dict={training_batch_size: batch_size, inference_batch_size: 500})
    train_handle, val_handle, test_handle = sess.run(
        [train_iterator.string_handle(), val_iterator.string_handle(), test_iterator.string_handle()])
    # Run the initializer
    for step in range(1, training_steps+1):
        sess.run([train_op], feed_dict={handle: train_handle})
        
        if step % display_step == 0 or step == 1:
            # Calculate batch loss and accuracy
            loss_local, acc, summary = sess.run([loss, accuracy, merged_summary_op], feed_dict={handle: train_handle})
            train_writer.add_summary(summary, global_step=step)
            train_writer.flush()            
            val_acc, val_loss = [], []
            for _ in range(20):
                tmp = sess.run([accuracy, loss], feed_dict={handle: val_handle})
                val_acc.append(tmp[0])
                val_loss.append(tmp[1])
            summary = tf.Summary()
            val_loss = np.mean(val_loss)            
            val_acc = np.mean(val_acc)            
            summary.value.add(tag='Loss', simple_value=val_loss)
            val_writer.add_summary(summary, global_step=step)
            val_writer.flush() 
            print("Step " + str(step) + ", Train Loss= " + \
                  "{:.4f}".format(loss_local) + ", Training Accuracy= " + \
                  "{:.3f}".format(acc) + ", Val Accuracy= " + \
                  "{:.3f}".format(val_acc))
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                save_path = saver.save(sess, "/tmp/model.ckpt")
                print("Model saved in path: %s" % save_path)  
    test_acc = []
    for _ in range(20):
        test_acc.append(sess.run(accuracy, feed_dict={handle: test_handle}))
    test_acc = np.mean(test_acc)
    print("Test Accuracy= {:.3f}".format(test_acc))                
    print("Optimization Finished!")
    
train_writer.close()
val_writer.close()

with tf.Session(graph=graph, config=config) as sess:
    saver.restore(sess, '/tmp/model.ckpt')
    sess.run(test_iterator.initializer,
             feed_dict={inference_batch_size: 500})
    test_handle = sess.run(test_iterator.string_handle())    
    test_acc = []
    for _ in range(20):
        test_acc.append(sess.run(accuracy, feed_dict={handle: test_handle}))
    test_acc = np.mean(test_acc)
    print("Test Accuracy= {:.3f}".format(test_acc))

"""## Permute"""

# Training Parameters
training_steps = 15000

tf.reset_default_graph()
graph = tf.Graph()
with graph.as_default():
    tf.set_random_seed(10)
    
    with tf.variable_scope("datasets"):
        training_batch_size = tf.placeholder(tf.int64) # tf.constant(32, dtype="int64")
        inference_batch_size = tf.placeholder(tf.int64) # tf.constant(500, dtype="int64")

        fminst_ds_train = tf.data.Dataset.from_tensor_slices(
            (df_train.iloc[:, 1:].astype("float32") / 255, df_train.iloc[:, 0].astype("int32"))
        ).shuffle(50000, reshuffle_each_iteration=True).repeat().batch(training_batch_size).map(process_batch)
        fminst_ds_val = tf.data.Dataset.from_tensor_slices(
            (df_val.iloc[:, 1:].astype("float32") / 255, df_val.iloc[:, 0].astype("int32"))
        ).repeat().batch(inference_batch_size).map(process_batch)
        fminst_ds_test = tf.data.Dataset.from_tensor_slices((
            df_test.iloc[:, 1:].astype("float32") / 255, df_test.iloc[:, 0].astype("int32"))
        ).repeat().batch(inference_batch_size).map(process_batch)

        handle = tf.placeholder(tf.string, shape=[])
        iterator = tf.data.Iterator.from_string_handle(
            handle, fminst_ds_train.output_types, fminst_ds_train.output_shapes)

        train_iterator = fminst_ds_train.make_initializable_iterator()
        val_iterator = fminst_ds_val.make_initializable_iterator()
        test_iterator = fminst_ds_test.make_initializable_iterator() 
    
    X_0, Y = iterator.get_next()
    
    # Permute the time step
    np.random.seed(100)
    permute = np.random.permutation(784)
    X = tf.gather(
        tf.reshape(X_0, (-1, timesteps, num_input)), 
        permute, axis=1)
    
    # Define weights
    logits = RNN(X)
    prediction = tf.nn.softmax(logits)
   
    with tf.name_scope("loss"):
        # Define loss and optimizer
        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(
            logits=logits, labels=Y))
        ema_loss = tf.get_variable("ema_loss", shape=[], dtype=tf.float32, trainable=False, initializer=tf.constant_initializer(2.5))
        ema_update = ema_loss.assign(ema_loss * 0.99 + loss * 0.01)
    tf.summary.scalar('Loss', ema_loss)
    
    with tf.variable_scope("optimizer"):
        # optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate)
        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.1)
        gvs = optimizer.compute_gradients(loss)
        capped_gvs = [(tf.clip_by_norm(grad, 1.), var) for grad, var in gvs]
        with tf.control_dependencies([ema_update]):
            train_op = optimizer.apply_gradients(capped_gvs)    
        # train_op = optimizer.minimize(loss_op)

    with tf.variable_scope("inference"):
        # Evaluate model (with test logits, for dropout to be disabled)
        correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))
        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))

    with tf.variable_scope('summarize_gradients'):
        for grad, var in gvs:
            norm = tf.norm(tf.clip_by_norm(grad, 10.), ord=2)
            tf.summary.histogram(var.name.replace(":", "_") + '/gradient_l2', 
                                 tf.where(tf.is_nan(norm), tf.zeros_like(norm), norm))
        for grad, var in capped_gvs:
            norm = tf.norm(grad, ord=2)
            tf.summary.histogram(var.name.replace(":", "_") + '/gradient_clipped_l2', 
                                 tf.where(tf.is_nan(norm), tf.zeros_like(norm), norm)) 
            
    # Initialize the variables (i.e. assign their default value)
    init = tf.global_variables_initializer()
    saver = tf.train.Saver()
    merged_summary_op = tf.summary.merge_all()
    # print([x.get_shape() for x in tf.global_variables()])
    # print([x.get_shape() for x in tf.trainable_variables()])
    # print("All parameters:", np.sum([np.product([xi.value for xi in x.get_shape()]) for x in tf.global_variables()]))
    # print("Trainable parameters:", np.sum([np.product([xi.value for xi in x.get_shape()]) for x in tf.trainable_variables()]))

# Start training
config = tf.ConfigProto()
config.gpu_options.allow_growth = True
best_val_acc = 0.8

train_writer = tf.summary.FileWriter("logs/fminst_gru/%s/train" % datetime.now().strftime("%Y%m%d_%H%M"), graph)
val_writer = tf.summary.FileWriter("logs/fminst_gru/%s/val" % datetime.now().strftime("%Y%m%d_%H%M"))

with tf.Session(graph=graph, config=config) as sess:
    sess.run(init) 
    sess.run([train_iterator.initializer, val_iterator.initializer, test_iterator.initializer],
             feed_dict={training_batch_size: batch_size, inference_batch_size: 500})
    train_handle, val_handle, test_handle = sess.run(
        [train_iterator.string_handle(), val_iterator.string_handle(), test_iterator.string_handle()])
    # Run the initializer
    for step in range(1, training_steps+1):
        sess.run([train_op], feed_dict={handle: train_handle})
        
        if step % display_step == 0 or step == 1:
            # Calculate batch loss and accuracy
            loss_local, acc, summary = sess.run([loss, accuracy, merged_summary_op], feed_dict={handle: train_handle})
            train_writer.add_summary(summary, global_step=step)
            train_writer.flush()            
            val_acc, val_loss = [], []
            for _ in range(20):
                tmp = sess.run([accuracy, loss], feed_dict={handle: val_handle})
                val_acc.append(tmp[0])
                val_loss.append(tmp[1])
            summary = tf.Summary()
            val_loss = np.mean(val_loss)            
            val_acc = np.mean(val_acc)            
            summary.value.add(tag='Loss', simple_value=val_loss)
            val_writer.add_summary(summary, global_step=step)
            val_writer.flush() 
            print("Step " + str(step) + ", Train Loss= " + \
                  "{:.4f}".format(loss_local) + ", Training Accuracy= " + \
                  "{:.3f}".format(acc) + ", Val Accuracy= " + \
                  "{:.3f}".format(val_acc))
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                save_path = saver.save(sess, "/tmp/model.ckpt")
                print("Model saved in path: %s" % save_path)  
    test_acc = []
    for _ in range(20):
        test_acc.append(sess.run(accuracy, feed_dict={handle: test_handle}))
    test_acc = np.mean(test_acc)
    print("Test Accuracy= {:.3f}".format(test_acc))                
    print("Optimization Finished!")
    
train_writer.close()
val_writer.close()

with tf.Session(graph=graph, config=config) as sess:
    saver.restore(sess, '/tmp/model.ckpt')
    sess.run(test_iterator.initializer,
             feed_dict={inference_batch_size: 500})
    test_handle = sess.run(test_iterator.string_handle())    
    test_acc = []
    for _ in range(20):
        test_acc.append(sess.run(accuracy, feed_dict={handle: test_handle}))
    test_acc = np.mean(test_acc)
    print("Test Accuracy= {:.3f}".format(test_acc))

